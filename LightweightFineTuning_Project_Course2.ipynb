{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f35354cd",
      "metadata": {
        "id": "f35354cd"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560fb3ff",
      "metadata": {
        "id": "560fb3ff"
      },
      "source": [
        "In this cell, describe your choices for each of the following\n",
        "\n",
        "* **PEFT technique**: LoRA (LoRA (Low-Rank Adaptation of Large Language Models) is a technique designed to fine-tune large pre-trained models, such as GPT or BERT, efficiently by freezing most of the model's parameters and training only a few added low-rank matrices._\n",
        "* **Model**: gpt2 (GPT-2 (Generative Pretrained Transformer 2) is an advanced language model developed by OpenAI. It is the second version of the Generative Pretrained Transformer series, designed to generate human-like text based on a given prompt.)\n",
        "* **Evaluation approach**: Evaluation before and after fine-tuning using the Trainer's evaluate() method. This approach provides a direct comparison of model performance before and after fine-tuning, ensuring the effectiveness of the fine-tuning process. By evaluating on the validation dataset using the same metrics and procedures, we can assess the impact of fine-tuning on model performance objectively.\n",
        "* **Fine-tuning dataset**: sms_spam (https://huggingface.co/datasets/sms_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8d76bb",
      "metadata": {
        "id": "de8d76bb"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f551c63a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f551c63a",
        "outputId": "7c3211a4-ea34-4db8-e718-bcc19526e5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install the required version of datasets in case you have an older version\n",
        "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
        "#!pip install -q \"datasets==2.15.0\"\n",
        "#!pip install transformers\n",
        "#!pip install peft\n",
        "!pip install datasets\n",
        "#!pip install pandas\n",
        "#!pip install numpy\n",
        "#!pip install scikit-learn\n",
        "#!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f28c4a78",
      "metadata": {
        "id": "f28c4a78"
      },
      "outputs": [],
      "source": [
        "# Import the 'load_dataset' function from the 'datasets' library\n",
        "# 'datasets' is a Hugging Face library that provides access to various public datasets for NLP tasks\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SMS spam dataset from Hugging Face datasets\n",
        "# The 'sms_spam' dataset contains labeled SMS messages as either \"ham\" (not spam) or \"spam\"\n",
        "# The 'split=\"train\"' argument retrieves the training portion of the dataset by default\n",
        "sms_spam_dataset = load_dataset(\"sms_spam\", split=\"train\")\n",
        "\n",
        "# Split the dataset into training and test sets using an 80-20 split\n",
        "# - test_size=0.2: 20% of the data will be used for testing\n",
        "# - shuffle=True: The data will be shuffled randomly before splitting to ensure both train and test sets are well-mixed\n",
        "# - seed=42: A fixed random seed ensures reproducibility of the data split, meaning the same shuffle will occur every time the code is run\n",
        "sms_spam_split = sms_spam_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e93c8ccb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e5eb3671a71e490692554d8274aa8840",
            "a53e8cecd9234a0aa09378f13ab67a5e",
            "fbc28213151d4dd3a24f12ef26a1cde2",
            "4e31befe91af47af8449b15525a6f8fc",
            "b95c6fe749b2470cbb7375732073d19c",
            "49cc9ec5e4574a8abddc837c2b717dc9",
            "ef1fec8c8f404a9cbb01e17e1f70d6f0",
            "ff2d911b63f7489796d0bb65533feb72",
            "dd42eb20f7f74c489bcbbc3670ac58ae",
            "86d0279e10a0418295ea5998ce9a8a89",
            "9a664aaa286748d4aa6de951f1ac9820",
            "b6a8163d2f824ebfa8d9c07d829c347d",
            "f43e13ed990148b1b4ed9c091cc430f3",
            "33967bdef3a34f7cb19dd227ba7bd9f4",
            "99d5138ff8d247f09e2db7ee36c7aeec",
            "5f12213d4fc9478eb840ad7718c27c8f",
            "b10e011dadb04f4a8696d9dcdcb3f59d",
            "c90fc31564e248b586e1a712c44d373f",
            "cfec1013ea3f4a428e6ba8b3ac0c210a",
            "f666e01324114c71be0bb5f82847c075",
            "1f3359a3b88542c0abb24307bac22def",
            "c826a6a324454471841242a04f94b56f"
          ]
        },
        "id": "e93c8ccb",
        "outputId": "77903f6a-e0fc-48d6-8b16-7f2f54a67419"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4459 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5eb3671a71e490692554d8274aa8840"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6a8163d2f824ebfa8d9c07d829c347d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import the AutoTokenizer class from the transformers library\n",
        "# The AutoTokenizer allows you to easily load pre-trained tokenizers for various transformer models.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for GPT-2, which is a pre-trained model for natural language processing tasks\n",
        "# The tokenizer will convert the input text into tokens that GPT-2 understands\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence (EOS) token for GPT-2\n",
        "# GPT-2 does not have a dedicated padding token, so we use the EOS token for padding purposes\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define a function to tokenize a batch of data\n",
        "# The function takes a batch (which is a dictionary) as input and tokenizes the \"sms\" field\n",
        "# - padding=True: Ensures all sequences are padded to the same length\n",
        "# - truncation=True: Ensures sequences longer than the model's maximum input length are truncated\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"sms\"], padding=True, truncation=True)\n",
        "\n",
        "# Tokenize the training set using the 'map' function to apply the 'tokenize' function to each batch\n",
        "# - batched=True: Processes the data in batches for efficiency, instead of one example at a time\n",
        "train_dataset = sms_spam_split[\"train\"].map(tokenize, batched=True)\n",
        "\n",
        "# Tokenize the test set in the same way as the training set\n",
        "test_dataset = sms_spam_split[\"test\"].map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1e9df5e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e9df5e8",
        "outputId": "90652916-1b7a-45a5-eba1-c26deea47940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Import the AutoModelForSequenceClassification class from the transformers library\n",
        "# This class allows us to load pre-trained transformer models for sequence classification tasks.\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the GPT-2 model pre-trained for sequence classification tasks\n",
        "# - num_labels=2: The model is being adapted for a binary classification task (spam or not spam)\n",
        "# - id2label: A mapping from label indices (0 and 1) to label names (\"not spam\" and \"spam\")\n",
        "# - label2id: A reverse mapping from label names to label indices\n",
        "# We are fine-tuning GPT-2 to classify SMS messages into spam or not spam\n",
        "foundation_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2,  # The dataset has two categories: \"spam\" and \"not spam\"\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},  # Mapping from numeric IDs to label names\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},  # Mapping from label names to numeric IDs\n",
        ")\n",
        "\n",
        "# Set the padding token ID for the model to match the tokenizer's padding token ID\n",
        "# Since GPT-2 uses its eos_token as the padding token, we make sure that the model's config reflects this\n",
        "foundation_model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca02945d",
      "metadata": {
        "id": "ca02945d"
      },
      "source": [
        "## Evaluating gpt2 foundation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c3712d97",
      "metadata": {
        "id": "c3712d97"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and modules\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "\n",
        "# Initialize lists to store predictions and true labels for evaluation\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the test dataset to make predictions for each example\n",
        "for example in test_dataset:\n",
        "    # Choose the device (GPU if available, otherwise fallback to CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the model to the selected device\n",
        "    foundation_model.to(device)\n",
        "\n",
        "    # Prepare the input text (SMS message) for the model\n",
        "    # 'return_tensors=\"pt\"' ensures the output is in PyTorch tensor format\n",
        "    inputs = tokenizer(example[\"sms\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Disable gradient calculation to save memory and computation (inference mode)\n",
        "    with torch.no_grad():\n",
        "        # Forward pass through the model to get raw logits (predictions before softmax)\n",
        "        outputs = foundation_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Apply the softmax function to convert logits into probabilities\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    # Get the predicted class with the highest probability\n",
        "    predicted_class_id = probabilities.argmax().item()\n",
        "\n",
        "    # Append the predicted class and true label to the respective lists\n",
        "    predictions.append(predicted_class_id)\n",
        "    labels.append(example[\"label\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9ec48bdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ec48bdf",
        "outputId": "146e3a8b-3385-4e34-b777-248a7bf263e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8619\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1 Score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# After collecting predictions and labels, you can compute the evaluation metrics\n",
        "# Compute accuracy, precision, recall, and F1-score to evaluate model performance\n",
        "accuracy = accuracy_score(labels, predictions)\n",
        "precision = precision_score(labels, predictions, average='binary')\n",
        "recall = recall_score(labels, predictions, average='binary')\n",
        "f1 = f1_score(labels, predictions, average='binary')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d52a229",
      "metadata": {
        "id": "4d52a229"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5775fadf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5775fadf",
        "outputId": "118e8086-976f-4d29-f672-5d229301a403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 148,992 || all params: 124,590,336 || trainable%: 0.1196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary classes from the PEFT (Parameter Efficient Fine-Tuning) library\n",
        "# LoraConfig: Configuration for LoRA (Low-Rank Adaptation) fine-tuning\n",
        "# PeftModelForSequenceClassification: Class to apply PEFT techniques to a sequence classification task\n",
        "# TaskType: Specifies the task type for the PEFT model (in this case, sequence classification)\n",
        "# AutoPeftModelForSequenceClassification: Automatically loads a pre-trained PEFT model for sequence classification\n",
        "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
        "\n",
        "# Define the PEFT model configuration\n",
        "# LoRA (Low-Rank Adaptation) is a technique to efficiently fine-tune pre-trained models by introducing low-rank adapters\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,  # Define the task as sequence classification (SEQ_CLS)\n",
        "    inference_mode=False,        # Set inference_mode to False, indicating we are in training mode\n",
        "    r=4,                         # 'r' is the rank of the low-rank matrix in LoRA, controlling the complexity of the adaptation\n",
        "    lora_alpha=16,               # 'lora_alpha' scales the low-rank updates, controlling the learning rate for LoRA adapters\n",
        "    lora_dropout=0.1             # 'lora_dropout' applies dropout to the LoRA adapters to prevent overfitting\n",
        ")\n",
        "\n",
        "# Load the pre-trained GPT-2 model and configure it for binary sequence classification (spam vs. not spam)\n",
        "# - num_labels=2: There are two possible labels (spam and not spam)\n",
        "# - id2label and label2id: Map the numeric label indices (0 and 1) to the corresponding string labels (\"not spam\" and \"spam\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2,  # Binary classification task\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},  # Mapping from label indices to label names\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},  # Mapping from label names to label indices\n",
        ")\n",
        "\n",
        "# Set the padding token to the eos_token for GPT-2 since it doesnâ€™t have a dedicated padding token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Apply PEFT (LoRA) to the model using the configuration defined above\n",
        "# This applies LoRA adapters to the pre-trained GPT-2 model for efficient fine-tuning\n",
        "peft_model = PeftModelForSequenceClassification(model, peft_config)\n",
        "\n",
        "# Print the trainable parameters of the PEFT model\n",
        "# This will give us a summary of the parameters that will be fine-tuned (LoRA adapters)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec7fb35",
      "metadata": {
        "id": "3ec7fb35"
      },
      "source": [
        "## PEFT model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "2627baef",
      "metadata": {
        "id": "2627baef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to compute evaluation metrics\n",
        "# The function receives `eval_pred` (a tuple containing predictions and labels) as input\n",
        "def compute_evaluation_metrics(eval_pred):\n",
        "    # Unpack the predictions and labels from the tuple\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Convert the logits (raw model outputs) to predicted class labels by taking the argmax across each prediction\n",
        "    # argmax(axis=1) selects the class with the highest probability (most likely class)\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Compute accuracy: The number of correct predictions divided by the total number of predictions\n",
        "    # The comparison (predictions == labels) results in a boolean array, where True is 1 and False is 0\n",
        "    accuracy = (predictions == labels).mean()\n",
        "\n",
        "    # Return the dictionary with the computed accuracy metric\n",
        "    return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4a65581a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "4a65581a",
        "outputId": "693bf10e-3fed-4418-bed5-c199c5c61570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-37-46d8e88ec916>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  peft_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='700' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [700/700 14:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.083900</td>\n",
              "      <td>1.327786</td>\n",
              "      <td>0.862780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.286100</td>\n",
              "      <td>0.583321</td>\n",
              "      <td>0.855605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.546100</td>\n",
              "      <td>0.403312</td>\n",
              "      <td>0.868161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.392800</td>\n",
              "      <td>0.330501</td>\n",
              "      <td>0.883408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.364400</td>\n",
              "      <td>0.313403</td>\n",
              "      <td>0.886099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=700, training_loss=0.8245353807721819, metrics={'train_runtime': 844.1601, 'train_samples_per_second': 26.411, 'train_steps_per_second': 0.829, 'total_flos': 2940033184856064.0, 'train_loss': 0.8245353807721819, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Import necessary classes from the Hugging Face Transformers library\n",
        "# DataCollatorWithPadding: Automatically pads the inputs in each batch to the maximum length in the batch\n",
        "# Trainer: Class responsible for training and evaluating the model\n",
        "# TrainingArguments: Configuration for training such as batch size, learning rate, etc.\n",
        "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "# Define the training arguments with a number of configurations to control the training process\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=\"./results/peft_model\",  # Directory to save model checkpoints and results\n",
        "    evaluation_strategy=\"epoch\",        # Evaluate the model after every epoch\n",
        "    learning_rate=2e-5,                 # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=32,     # Batch size used during training for each device (GPU/CPU)\n",
        "    per_device_eval_batch_size=32,      # Batch size used during evaluation for each device\n",
        "    num_train_epochs=5,                 # Number of training epochs\n",
        "    weight_decay=0.01,                  # Weight decay to prevent overfitting (L2 regularization)\n",
        "    logging_dir='./logs/peft_model',    # Directory for saving logs\n",
        "    save_strategy=\"epoch\",              # Save the model checkpoint after every epoch\n",
        "    load_best_model_at_end=True,        # Load the best model (based on evaluation) at the end of training\n",
        "    logging_steps=100,                  # Log training metrics every 100 steps\n",
        "    warmup_ratio=0.1,                   # Proportion of training steps to perform learning rate warmup\n",
        ")\n",
        "\n",
        "# Initialize the Trainer class, which is responsible for training the model\n",
        "# The Trainer will take care of running the training loop, logging, evaluation, and saving the model\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,                  # The model being trained\n",
        "    args=peft_training_args,           # The training configuration\n",
        "    train_dataset=train_dataset,       # The dataset to be used for training\n",
        "    eval_dataset=test_dataset,         # The dataset to be used for evaluation\n",
        "    compute_metrics=compute_evaluation_metrics,   # Function to compute evaluation metrics after each evaluation\n",
        "    tokenizer=tokenizer,               # The tokenizer used to encode/decode text inputs\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Automatically pad inputs to the same length in a batch\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "# This will start the training loop with the provided configurations\n",
        "peft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d2efa847",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "d2efa847",
        "outputId": "8ce97702-ad50-4c58-bfaa-c096bfbd6e65"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [35/35 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.3134034276008606,\n",
              " 'eval_accuracy': 0.8860986547085202,\n",
              " 'eval_runtime': 11.1442,\n",
              " 'eval_samples_per_second': 100.052,\n",
              " 'eval_steps_per_second': 3.141,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Evaluate the model on the evaluation dataset (test dataset in this case)\n",
        "# The `evaluate()` function computes the evaluation metrics defined in `compute_metrics`\n",
        "# and returns the results (e.g., accuracy, loss, etc.) as a dictionary\n",
        "evaluation_results_peft = peft_trainer.evaluate()\n",
        "\n",
        "# Print the evaluation results, which will include the metrics calculated during evaluation (like accuracy)\n",
        "evaluation_results_peft"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791d9dd2",
      "metadata": {
        "id": "791d9dd2"
      },
      "source": [
        "## Save PEFT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f432f53f",
      "metadata": {
        "id": "f432f53f"
      },
      "outputs": [],
      "source": [
        "# Save the trained PEFT model to a specified directory\n",
        "# This saves the model's weights, configuration, and tokenizer (if needed) to the directory\n",
        "peft_model.save_pretrained('models/peft_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615b12c6",
      "metadata": {
        "id": "615b12c6"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "863ec66e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863ec66e",
        "outputId": "2ee7e459-9de3-4d90-e6fb-f82e031cd249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary classes for PEFT (LoRA) model loading\n",
        "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
        "\n",
        "# Load the PEFT model for inference (the model saved previously)\n",
        "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
        "    \"models/peft_model\",  # Path to the saved model directory\n",
        "    num_labels=2,         # Number of output labels (e.g., spam and not spam)\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},  # Mapping from label ids to label names\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},  # Mapping from label names to label ids\n",
        ")\n",
        "\n",
        "# Set pad_token_id to eos_token_id (this ensures that padding uses the same token as the end-of-sequence token)\n",
        "inference_model.config.pad_token_id = inference_model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "bc96905a",
      "metadata": {
        "id": "bc96905a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_label(prompt: str) -> str:\n",
        "    # Check if a GPU (CUDA) is available, otherwise use the CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the model to the selected device (GPU or CPU)\n",
        "    inference_model.to(device)\n",
        "\n",
        "    # Prepare the input text (tokenize the prompt)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # Get predictions (disable gradient tracking during inference)\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Apply softmax to convert logits to probabilities\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    # Get the predicted class ID (class with the highest probability)\n",
        "    predicted_class_id = probabilities.argmax(dim=1).item()\n",
        "\n",
        "    # Mapping from class ID to label name (spam or not spam)\n",
        "    id2label = {0: \"spam\", 1: \"not spam\"}\n",
        "\n",
        "    # Get the predicted label based on the predicted class ID\n",
        "    predicted_label = id2label.get(predicted_class_id, \"Unknown\")\n",
        "\n",
        "    return predicted_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "866ab28c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866ab28c",
        "outputId": "63c9d69b-a6c0-41ee-b3f3-c87dced32342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.'\n",
            "Predicted label: spam\n"
          ]
        }
      ],
      "source": [
        "# Test the prediction with a sample prompt\n",
        "sample_prompt = \"FREE!FREE!FREE Get yous eye checkd for free bya renowned eye specialist of the city\"\n",
        "\n",
        "# Output the result\n",
        "print(f\"Prompt: '{prompt}'\\nPredicted label: {predict_label(sample_prompt)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834deda0",
      "metadata": {
        "id": "834deda0"
      },
      "source": [
        "# Conclusion:\n",
        "## Comparision the performance of the PEFT model with that of the original foundational GPT2 model.\n",
        "\n",
        "A traditional approach to fine-tuning Large Language Models (LLMs) typically involves adjusting the majority of the model's weights, which demands significant computational resources. In contrast, LoRA-based fine-tuning offers a more efficient alternative by freezing the original weights and training only a small set of additional parameters, making the process much more resource-efficient. When comparing the performance of the original foundational model with the PEFT (Parameter-Efficient Fine-Tuning) model, the PEFT model demonstrates higher accuracy.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5eb3671a71e490692554d8274aa8840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a53e8cecd9234a0aa09378f13ab67a5e",
              "IPY_MODEL_fbc28213151d4dd3a24f12ef26a1cde2",
              "IPY_MODEL_4e31befe91af47af8449b15525a6f8fc"
            ],
            "layout": "IPY_MODEL_b95c6fe749b2470cbb7375732073d19c"
          }
        },
        "a53e8cecd9234a0aa09378f13ab67a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49cc9ec5e4574a8abddc837c2b717dc9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef1fec8c8f404a9cbb01e17e1f70d6f0",
            "value": "Map:â€‡100%"
          }
        },
        "fbc28213151d4dd3a24f12ef26a1cde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2d911b63f7489796d0bb65533feb72",
            "max": 4459,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd42eb20f7f74c489bcbbc3670ac58ae",
            "value": 4459
          }
        },
        "4e31befe91af47af8449b15525a6f8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86d0279e10a0418295ea5998ce9a8a89",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a664aaa286748d4aa6de951f1ac9820",
            "value": "â€‡4459/4459â€‡[00:01&lt;00:00,â€‡3529.10â€‡examples/s]"
          }
        },
        "b95c6fe749b2470cbb7375732073d19c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49cc9ec5e4574a8abddc837c2b717dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1fec8c8f404a9cbb01e17e1f70d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2d911b63f7489796d0bb65533feb72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd42eb20f7f74c489bcbbc3670ac58ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d0279e10a0418295ea5998ce9a8a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a664aaa286748d4aa6de951f1ac9820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6a8163d2f824ebfa8d9c07d829c347d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f43e13ed990148b1b4ed9c091cc430f3",
              "IPY_MODEL_33967bdef3a34f7cb19dd227ba7bd9f4",
              "IPY_MODEL_99d5138ff8d247f09e2db7ee36c7aeec"
            ],
            "layout": "IPY_MODEL_5f12213d4fc9478eb840ad7718c27c8f"
          }
        },
        "f43e13ed990148b1b4ed9c091cc430f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b10e011dadb04f4a8696d9dcdcb3f59d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c90fc31564e248b586e1a712c44d373f",
            "value": "Map:â€‡100%"
          }
        },
        "33967bdef3a34f7cb19dd227ba7bd9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfec1013ea3f4a428e6ba8b3ac0c210a",
            "max": 1115,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f666e01324114c71be0bb5f82847c075",
            "value": 1115
          }
        },
        "99d5138ff8d247f09e2db7ee36c7aeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3359a3b88542c0abb24307bac22def",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c826a6a324454471841242a04f94b56f",
            "value": "â€‡1115/1115â€‡[00:00&lt;00:00,â€‡3631.42â€‡examples/s]"
          }
        },
        "5f12213d4fc9478eb840ad7718c27c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b10e011dadb04f4a8696d9dcdcb3f59d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90fc31564e248b586e1a712c44d373f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfec1013ea3f4a428e6ba8b3ac0c210a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f666e01324114c71be0bb5f82847c075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f3359a3b88542c0abb24307bac22def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c826a6a324454471841242a04f94b56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
